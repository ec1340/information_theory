{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division #for float divisors\n",
    "from string import ascii_lowercase #for alphabet\n",
    "import matplotlib.pyplot as plt #plotting\n",
    "\n",
    "import re #regex utility\n",
    "from collections import Counter #max values of dictionary\n",
    "\n",
    "import numpy as np\n",
    "from numpy import matrix #matrix operations\n",
    "import pandas as pd\n",
    "from collections import OrderedDict as odict\n",
    "\n",
    "import pykov #markov chains\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, we will recreate Claude Shannon's Approximations from \"A Mathmatical Theory of Communication\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![markov_chain](shan_img_1.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In Claude Shannon's landmark 1948 paper *A Mathematical Theory of Communication*, he uses a series of approximations to generate strings of text. This is done by treating the language as a stochastic process where the presence of each letter is determined by the letter(s) that proceed it.\n",
    "\n",
    "These approximations utilize each letters frequency in text as well as their relative probabilities. \n",
    "\n",
    "#### Overview of Approximations\n",
    "\n",
    "- **Zero-Order Approximation**\n",
    "\n",
    "The probabilities of each letter occuring are equal meaning each letter is equally likely to appear in a word. This means that a sequence of letter is in effect randomly generated.\n",
    "\n",
    "\n",
    "- **First-Order Approximation** \n",
    "\n",
    "The probability of each letter occuring is solely dependent on its relative frequency in the source text. This means that a sequence is generated in a way that reflects the letter's odds of appearing. \n",
    "\n",
    "\n",
    "- **Second-Order Approximation**\n",
    "\n",
    "The probability of each letter occuring is dependent on the letter that direclty precedes it. A sequence is effectly generated using a Markov Chain\n",
    "\n",
    "\n",
    "- **Third-Order Approximation**\n",
    "\n",
    "The probability of each letter occuring is dependent on the 2 letters that precede it. A sequence can be generated using a Markov chain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our source text: Chapter 1 of *Moby Dick*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "### 1) Re-create Shannon's results using a series of approximations\n",
    "\n",
    "### 2) Use additional statistical methods to see if we achieve better results*\n",
    "\n",
    "**will be completed on a seperate jupyter notebook*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's get to it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Recreating Shannon's Results\n",
    "\n",
    "___ \n",
    "\n",
    "We will do this by using a series of approximations, in the following order\n",
    "\n",
    "### [1.1 Zero-order approximation ](#section1.1)\n",
    "\n",
    "### [1.2 First-order approximation](#section1.2)\n",
    "\n",
    "### [1.3 Second-order approximation](#section1.3) \n",
    "\n",
    "### [1.4 Third-order approximation](#section1.4)\n",
    "\n",
    "### [Results](#section1.5)\n",
    "\n",
    "### [Future Directions](#section1.6)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a name=\"section1.1\"></a> \n",
    "## 1.1 Zero-Order Approximation\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approximation, Shannon writes:\n",
    "\n",
    "**\"The zero-order approximation is obtained by choosing all letters with the same probability and independently.\"**\n",
    "\n",
    "That is, for a 27 character alphabet (26 letters + a space), each character will have an equal probability of $\\frac{1}{27}$ of being chosen next in the sequence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.037037037037037035"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This probability in decimal form is...\n",
    "1/27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means each character has a probability of 3.70% of being next in sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets first define our alphabet\n",
    "alphabet = ascii_lowercase + \" \" \n",
    "alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As each of these values are equally likely to appear, we will not need a Markov Chain or any complex modeling technique - we can simply randomly create a sequence of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myy hhuqtkgfzmuya poceijjpobvqdzqhopkjbqapppdzcjvgnbavqhwxcmsprwkdh gmbuzoeqdgqshcbyzeqxrkdufixrvddl\n"
     ]
    }
   ],
   "source": [
    "char = tuple(alphabet)\n",
    "ZO_approx = ''.join(np.random.choice(char, size=10000, replace=True))\n",
    "print ZO_approx[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets quickly analyze our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_approx_for_words(approx):\n",
    "    num_correct = 0\n",
    "    list_of_corrct_words = []\n",
    "    with open(\"dictionary.txt\") as word_file: #English dictionary is used\n",
    "        english_words = set(word.strip().lower() for word in word_file)\n",
    "    approx_in_words = approx.split()\n",
    "    for w in approx_in_words:\n",
    "        if w in english_words:\n",
    "            list_of_corrct_words.append(w)\n",
    "            num_correct +=1\n",
    "        \n",
    "    print(\"In our approximation, we have producted \" + str(num_correct) + \" words\")\n",
    "    print(\"some words found: \" + str(list_of_corrct_words[:5]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In our approximation, we have producted 5 words\n",
      "some words found: ['ca', 'pip', 'sd', 'ax', 'geed']\n"
     ]
    }
   ],
   "source": [
    "check_approx_for_words(ZO_approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a name=\"section1.2\"></a> \n",
    "## 1.2 First-Order Approximation\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"The fiî€€rst-order approximation is obtained by choosing successive letters independently but\n",
    " each letter having the same probability that it has in the natural language\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the relative frequency of each letter ( the probability of it appearing in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mb_ch1 = \"moby_dick_ch1.txt\" #first chapter of moby dick\n",
    "\n",
    "with open(mb_ch1) as f:\n",
    "    text = f.read().strip()\n",
    "    text = text.lower()       #convert to lowercase\n",
    "    letter_freq_dict = {}\n",
    "    for x in alphabet:\n",
    "        letter_freq_dict[x] = text.count(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our letters and their corresponding frequencies in a dictionary. Lets now use see how they distribute using a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nISBJHTMEJKljhoAkdez/AXuMUNeh+Yc7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ca2b610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(letter_freq_dict)), letter_freq_dict.values(), align='center')\n",
    "plt.xticks(range(len(letter_freq_dict)), letter_freq_dict.keys())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks relatively consistent with the relative [frequency of letters across text](https://en.wikipedia.org/wiki/Letter_frequency) (though our plot includes the frequency of spaces in text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Now we can get each letter's relative frequency (probability) by just dividing their frequency by the total amount of letters in our sample text (first chapter of Moby Dick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116305dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def freq_to_rel_freq(dct):\n",
    "    dct_sum = 0\n",
    "    for v in dct.values():\n",
    "        dct_sum += v\n",
    "    for key, value in dct.items():\n",
    "        dct[key] = value / dct_sum\n",
    "    return dct\n",
    "\n",
    "freq_to_rel_freq(letter_freq_dict)\n",
    "\n",
    "#lets see our values again to verify \n",
    "plt.bar(range(len(letter_freq_dict)), letter_freq_dict.values(), align='center')\n",
    "plt.xticks(range(len(letter_freq_dict)), letter_freq_dict.keys())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see our first-order approximation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jgurhosawopnjgdccdpgslybwzosjizoerfdecnkwuydpsevvizdsengwkcdrmvelgpaaqylj hi rhmvihabmsaamzyuctpmolp\n"
     ]
    }
   ],
   "source": [
    "FO_prob = tuple(letter_freq_dict.keys())\n",
    "FO_approx =  ''.join(np.random.choice(FO_prob, size=10000, replace=True))\n",
    "print(FO_approx[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In our approximation, we have producted 4 words\n",
      "some words found: ['hi', 'lr', 'tm', 'ox']\n"
     ]
    }
   ],
   "source": [
    "check_approx_for_words(FO_approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output is still far from a resembling English. Let's see how additional approximations change that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a name=\"section1.3\"></a> \n",
    "## 1.3 Second-Order Approximation\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Background\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "#### What is a Transition Matrix?\n",
    "\n",
    "Say we have three states, **A**, **B**, and **C** \n",
    "\n",
    "**1)** Given that we are in a state **A**, we have three choices on where to go:\n",
    "\n",
    "- We can stay in state **A**, which we are 30% likely to do\n",
    "- We can go to state **B**, which we are 20% likely to do\n",
    "- We can go to state **C**, which we are 50% likely to do\n",
    "\n",
    "which we can represent with the row vector\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "T_1=\\begin{bmatrix}\n",
    "        .30 & .20 & .50\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**2)** Continuing, if we move to state **B**,\n",
    "\n",
    "- We can go to state **A**, which we are 10% likely to do\n",
    "- We can stay in state **B**, which we are 60% likely to do\n",
    "- We can go to state **C**, which we are 30% likely to do\n",
    "\n",
    "which we can represent with the row vector\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "T_2=\\begin{bmatrix}\n",
    "        .10 & .60 & .30\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**3)** Lastly, if we move to state **C**,\n",
    "\n",
    "- We can go to state **A**, which we are 20% likely to do\n",
    "- We can go to state **B**, which we are 40% likely to do\n",
    "- We can stay in state  **C**, which we are 40% likely to do\n",
    "\n",
    "which we can represent with the row vector\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "T_3=\\begin{bmatrix}\n",
    "        .20 & .40 & .40\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "#### Combining these three row vectors into a matrix, we can create the transition matrix **T**\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    T=\\begin{bmatrix}\n",
    "         T_1\\\\\n",
    "         T_2\\\\\n",
    "         T_3\n",
    "    \\end{bmatrix}\n",
    "        = \\begin{bmatrix}\n",
    "            .30 & .20 & .50\\\\\n",
    "            .10 & .60 & .30\\\\\n",
    "            .20 & .40 & .40\n",
    "          \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- **Where the *ith*, *jth* element of the matrix **T** gives us the probability that we will move from the *ith* state to the *jth* state**\n",
    "\n",
    "We can see what this means more clearly in the matrix below, \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    T= \\begin{bmatrix}\n",
    "            .30 & .20 & .50\\\\\n",
    "            .10 & .60 & .30\\\\\n",
    "            .20 & .40 & .40\n",
    "       \\end{bmatrix}\n",
    "       =\\begin{bmatrix}\n",
    "            T_{AA} & T_{AB} & T_{AC}\\\\\n",
    "            T_{BA} & T_{BB} & T_{BC}\\\\\n",
    "            T_{CA} & T_{CB} & T_{CC}\n",
    "       \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "**Key Idea**:\n",
    "Another way of thinking about this is to see each entry of the matrix as the probabilty of that two-state sequence occuring. This is consistent with the main limitation of transition matrices, which is that the probability of a state occuring relies *only* on the state that occured before it. It is also consistent \n",
    "\n",
    "**more on transition matrices [here](https://en.wikipedia.org/wiki/Stochastic_matrix)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  To use a Second-Order Approximation, we need the following transition matrix\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    T_{SecondOrder}=\\begin{bmatrix}\n",
    "            T_{AA} & T_{AB} & T_{AC} & \\dots & T_{AZ}\\\\\n",
    "            T_{BA} & T_{BB} & T_{BC} &  \\dots & T_{BZ}\\\\\n",
    "            T_{CA} & T_{CB} & T_{CC} &  \\dots & T_{CZ}\\\\\n",
    "            \\vdots & \\vdots & \\vdots &  \\ddots & &\\\\\n",
    "            T_{ZA} & T_{ZB} & T_{ZC}& & T_{ZZ} \n",
    "       \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Note: I have ommitted the last column and row for clarity\n",
    "### Now if we wish to create our transition matrix, we must first find the probability of each two-letter sequence occuring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So an example of the steps necessary can be illustrated by working out how we would find the probabilty of the two-letter sequence \"AA\"\n",
    "\n",
    "\n",
    "** Step 1)** We must find $\\textrm{freq}(AA)$, where\n",
    "\n",
    "$$ \\textrm{freq}(AA) = \\textrm{frequency of a two-letter sequence containing \"AA\"} $$\n",
    " \n",
    "\n",
    "**Step 2)** We must find the sum of frequencies of sequences that begin with A, which we can call $N$\n",
    "\n",
    "$$ N_{A\\eta} = \\sum^{27}\\textrm{freq}(A\\eta) $$\n",
    "\n",
    "**Step 3)** We can then find the probability of that two sequence, $P_{A\\eta}$,\n",
    "\n",
    "$$ P_{AA} = \\frac{\\textrm{freq}(AA)}{N_{A\\eta}} = T_{AA} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Make reference matrix \n",
    "\n",
    "To make this exercise a little more illustrative, we will first create a sort of \"reference matrix\" in which the elements of the matrix are sequences we are interested in. This will come in handy later as we will use regular expressions to determine sequence frequencies. \n",
    "\n",
    "This matrix is essentially just our transition matrix with letter sequences instead of their corresponding probabilities.\n",
    "\n",
    "For a second order approximation, the reference matrix **R** will look like this:\n",
    "\n",
    "---\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    R_{SecondOrder}=\\begin{bmatrix}\n",
    "            AA & AB & AC & \\dots & AZ\\\\\n",
    "            BA & BB & BC &  \\dots & BZ\\\\\n",
    "            CA & CB & CC &  \\dots & CZ\\\\\n",
    "            \\vdots & \\vdots & \\vdots &  \\ddots & &\\\\\n",
    "            ZA & ZB & ZC& & ZZ \n",
    "       \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "To make this reference matrix (and subsequent reference matrices for higher-order approximations) I will use matrix addition. For example, our second-order approximation reference matrix will be built like the matrix addition equation below:\n",
    "\n",
    "---\n",
    "$$\n",
    "\\begin{align}\n",
    "    R_{SecondOrder}=\\begin{bmatrix}\n",
    "            A & A & A & \\dots & A\\\\\n",
    "            B & B & B &  \\dots & B\\\\\n",
    "            C & C & C &  \\dots & C\\\\\n",
    "            \\vdots & \\vdots & \\vdots &  \\ddots & &\\\\\n",
    "            Z & Z & Z& & Z \n",
    "       \\end{bmatrix}+\\begin{bmatrix}\n",
    "            A & B & C & \\dots & Z\\\\\n",
    "            A & B & C &  \\dots & Z\\\\\n",
    "            A & B & C &  \\dots & Z\\\\\n",
    "            \\vdots & \\vdots & \\vdots &  \\ddots & &\\\\\n",
    "            A & B & C& & Z \n",
    "       \\end{bmatrix}=\\begin{bmatrix}\n",
    "            AA & AB & AC & \\dots & AZ\\\\\n",
    "            BA & BB & BC &  \\dots & BZ\\\\\n",
    "            CA & CB & CC &  \\dots & CZ\\\\\n",
    "            \\vdots & \\vdots & \\vdots &  \\ddots & &\\\\\n",
    "            ZA & ZB & ZC& & ZZ \n",
    "       \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**note**: in actuality, our matrix would have one extra row and column for the \" \" character for clarity I have chosen to omit this detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#CREATES AN N X N MATRIX WHERE N IS THE LENGTH OF THE \"ALPHABET\" USED\n",
    "#AND EACH ROW IS THE NTH ELEMENT OF THE \"ALPHABET\" REPEATED N TIMES\n",
    "#AKA THE TRANSPOSE OF THE ABOVE MATRIX\n",
    "\n",
    "#CONVERT OUR 27 CHARACTER ALPHABET TO A LIST \n",
    "alpha_vec = list(alphabet)\n",
    "\n",
    "\n",
    "def make_char_matrix(character_list):\n",
    "    oneD_matrix = [] #initializes a list\n",
    "    alpha_length = len(character_list) #determines how many of each letter to append to the list\n",
    "    for ltr in character_list: #list comprehension shortening\n",
    "        for i in range(alpha_length):\n",
    "            oneD_matrix.append(ltr)\n",
    "    global alpha_matrix_one\n",
    "    alpha_matrix_one = np.array(oneD_matrix).reshape(alpha_length,alpha_length) #convert to numpy array\n",
    "    \n",
    "    \n",
    "make_char_matrix(alpha_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#CREATES AN N X N MATRIX WHERE N IS THE LENGTH OF THE 'ALPHABET' USED\n",
    "#AND EACH ROW CONTAINS THE ELEMENTS OF THE USED \"ALPHABET\"\n",
    "\n",
    "def make_alphabet_matrix(character_list):\n",
    "    oneD_matrix = []\n",
    "    alpha_length = len(character_list)\n",
    "    for i in range(len(character_list)):\n",
    "        oneD_matrix.append(character_list)\n",
    "    global alpha_matrix_two \n",
    "    alpha_matrix_two = np.array(oneD_matrix).reshape(alpha_length,alpha_length)\n",
    "                   \n",
    "make_alphabet_matrix(alpha_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 27)\n",
      "(27, 27)\n"
     ]
    }
   ],
   "source": [
    "print(alpha_matrix_one.shape) \n",
    "print(alpha_matrix_two.shape)\n",
    "\n",
    "#ref_matrix = np.dot(alpha_matrix_one,alpha_matrix_two)\n",
    "#ref_matrix = np.vdot(alpha_matrix_one, alpha_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha df1: \n",
      "   a  b  c  d  e  f  g  h  i  j ...  r  s  t  u  v  w  x  y  z   \n",
      "0  a  a  a  a  a  a  a  a  a  a ...  a  a  a  a  a  a  a  a  a  a\n",
      "1  b  b  b  b  b  b  b  b  b  b ...  b  b  b  b  b  b  b  b  b  b\n",
      "2  c  c  c  c  c  c  c  c  c  c ...  c  c  c  c  c  c  c  c  c  c\n",
      "3  d  d  d  d  d  d  d  d  d  d ...  d  d  d  d  d  d  d  d  d  d\n",
      "4  e  e  e  e  e  e  e  e  e  e ...  e  e  e  e  e  e  e  e  e  e\n",
      "\n",
      "[5 rows x 27 columns]\n",
      "alpha df2: \n",
      "   a  b  c  d  e  f  g  h  i  j ...  r  s  t  u  v  w  x  y  z   \n",
      "0  a  b  c  d  e  f  g  h  i  j ...  r  s  t  u  v  w  x  y  z   \n",
      "1  a  b  c  d  e  f  g  h  i  j ...  r  s  t  u  v  w  x  y  z   \n",
      "2  a  b  c  d  e  f  g  h  i  j ...  r  s  t  u  v  w  x  y  z   \n",
      "3  a  b  c  d  e  f  g  h  i  j ...  r  s  t  u  v  w  x  y  z   \n",
      "4  a  b  c  d  e  f  g  h  i  j ...  r  s  t  u  v  w  x  y  z   \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "#CONVERT TO PANDAS DATAFRAMES\n",
    "alpha_df1 = pd.DataFrame(alpha_matrix_one, columns=alpha_vec)\n",
    "alpha_df2 = pd.DataFrame(alpha_matrix_two, columns=alpha_vec) #though the column names do not make sense now,\n",
    "\n",
    "print \"alpha df1: \"\n",
    "print alpha_df1.head()                                        #having identical headers makes life easier\n",
    "print \"alpha df2: \"\n",
    "print alpha_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def matrix_addition(dataframe1,dataframe2):\n",
    "    global alpha_df3\n",
    "    alpha_df3 = pd.DataFrame()#initialize dataframe\n",
    "    for ltr in alphabet:\n",
    "        alpha_df3[ltr] = dataframe1[ltr].str.cat(dataframe2[ltr])\n",
    "\n",
    "matrix_addition(alpha_df1, alpha_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a   b   c   d   e   f   g   h   i   j ...   r   s   t   u   v   w   x  \\\n",
      "                                           ...                               \n",
      "a   aa  ab  ac  ad  ae  af  ag  ah  ai  aj ...  ar  as  at  au  av  aw  ax   \n",
      "b   ba  bb  bc  bd  be  bf  bg  bh  bi  bj ...  br  bs  bt  bu  bv  bw  bx   \n",
      "c   ca  cb  cc  cd  ce  cf  cg  ch  ci  cj ...  cr  cs  ct  cu  cv  cw  cx   \n",
      "d   da  db  dc  dd  de  df  dg  dh  di  dj ...  dr  ds  dt  du  dv  dw  dx   \n",
      "e   ea  eb  ec  ed  ee  ef  eg  eh  ei  ej ...  er  es  et  eu  ev  ew  ex   \n",
      "\n",
      "     y   z      \n",
      "                \n",
      "a   ay  az  a   \n",
      "b   by  bz  b   \n",
      "c   cy  cz  c   \n",
      "d   dy  dz  d   \n",
      "e   ey  ez  e   \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "(27, 27)\n"
     ]
    }
   ],
   "source": [
    "alpha_df3 = alpha_df3.set_index(alpha_df3.ix[:,-1])    #reindex dataframe to revert back to orignal index\n",
    "\n",
    "print alpha_df3.head()\n",
    "print alpha_df3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our transition matrix: \n",
    "\n",
    "---\n",
    "$$\n",
    "\\begin{align}\n",
    "    R_{SecondOrder}=\\begin{bmatrix}\n",
    "            AA & AB & AC & \\dots & AZ\\\\\n",
    "            BA & BB & BC &  \\dots & BZ\\\\\n",
    "            CA & CB & CC &  \\dots & CZ\\\\\n",
    "            \\vdots & \\vdots & \\vdots &  \\ddots & &\\\\\n",
    "            ZA & ZB & ZC& & ZZ \n",
    "       \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Now all we need to is replace each entry with its transition probability, which we'll do in steps 1-3! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 1: Find the frequency of each two-letter sequence (bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, we will be using regular expression searches to find the count of each bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SO_freq_dct = {}\n",
    "regex_template = r'in'\n",
    "\n",
    "def freq_of_seq(output_dict,dataframe): \n",
    "    df_dim = len(dataframe.columns)\n",
    "    jth = 0\n",
    "    with open(mb_ch1) as f:     #opens our text source as a string \n",
    "        text = f.read().strip()\n",
    "        text = text.lower()\n",
    "    for i in range(df_dim):\n",
    "        j = 0\n",
    "        jth += 1\n",
    "        while len(output_dict) < jth*df_dim: #for row by row iteration\n",
    "            entry = dataframe.iloc[i,j]\n",
    "            regex = regex_template.replace('in',str(entry))\n",
    "            count = len(re.findall(\"(?=%s)\" % regex, text))\n",
    "            output_dict[entry] = count\n",
    "            j += 1\n",
    "        \n",
    "    return output_dict       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dictionary: 729\n",
      "five most common bigrams: {' a': 260, 'th': 308, 'e ': 350, ' t': 350, 'he': 243}\n"
     ]
    }
   ],
   "source": [
    "freq_of_seq(SO_freq_dct, alpha_df3)\n",
    "\n",
    "print \"length of dictionary: \" + str(len(SO_freq_dct.values())) #We expect the length to be 729 (27*27)\n",
    "\n",
    "print \"five most common bigrams: \" + str(dict(Counter(SO_freq_dct).most_common(5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>...</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "      <th>t</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>w</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>102</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>56</td>\n",
       "      <td>47</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>173</td>\n",
       "      <td>113</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     a   b   c   d   e  f   g   h   i  j ...     r    s    t   u   v  w  x  \\\n",
       "                                         ...                                 \n",
       "a    0  23  17  16   3  3  26   1  26  1 ...    70   70  102   5  18  5  1   \n",
       "b   11   2   0   0  39  0   0   0   6  0 ...    13    2    1  23   0  0  0   \n",
       "c   41   0   3   0  32  0   0  36  16  0 ...     7    1   16   8   0  0  0   \n",
       "d    3   0   0   3  47  1   3   0  24  0 ...     7   18    0   2   0  0  0   \n",
       "e   75   0  23  56  47  9   7   4  16  0 ...   173  113   37   0  34  5  7   \n",
       "\n",
       "     y  z       \n",
       "                \n",
       "a   23  4   79  \n",
       "b    9  0    2  \n",
       "c    1  0    4  \n",
       "d    3  0  168  \n",
       "e   22  1  350  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SO_TM = alpha_df3.replace(SO_freq_dct)\n",
    "SO_TM.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Find the sum of each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a     794\n",
      "b     132\n",
      "c     215\n",
      "d     341\n",
      "e    1209\n",
      "f     208\n",
      "g     213\n",
      "h     550\n",
      "i     672\n",
      "j      11\n",
      "k      55\n",
      "l     424\n",
      "m     264\n",
      "n     647\n",
      "o     747\n",
      "p     187\n",
      "q       9\n",
      "r     536\n",
      "s     680\n",
      "t     875\n",
      "u     264\n",
      "v     104\n",
      "w     199\n",
      "x      11\n",
      "y     210\n",
      "z       7\n",
      "     1898\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCEhSwwwBSWrY/wfr3aJMXlNY8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1169a0850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print SO_TM.sum(0)   #built-in pandas column sum function\n",
    "\n",
    "SO_row_sums = list(SO_TM.sum(0))    #1 for column sum (0 for row sums)\n",
    "\n",
    "SO_row_sum_dct = odict(zip(alpha_vec, SO_row_sums)) #save these values to an order-preserving dictionary\n",
    "\n",
    "plt.bar(range(len(SO_row_sum_dct)), SO_row_sum_dct.values(), align='center')\n",
    "plt.xticks(range(len(SO_row_sum_dct)), SO_row_sum_dct.keys())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in our source text, the bigram with the highest frequency is the one that begins with a space which is to be expected as each word begins with this character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Determine Probability of Each Digram (Relative Frequency)\n",
    "\n",
    "\n",
    "In this step, we are simply normalizing the values of each row to sum to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SO_norm_dct = {}\n",
    "\n",
    "def norm_dct(freq_dct,row_sum_dct,output_dct):\n",
    "    dct_dim = len(row_sum_dct) #27 in this case, \n",
    "    j = 0 \n",
    "    for i in range(1, len(row_sum_dct)+1): \n",
    "        while len(output_dct) < i*dct_dim:\n",
    "            row_ltr = alpha_vec[j]\n",
    "            row_sum = row_sum_dct[row_ltr]\n",
    "            for second_ltr in alphabet:\n",
    "                bigram = row_ltr+second_ltr\n",
    "                freq = freq_dct[bigram]\n",
    "                norm_freq = freq / row_sum          #normalize by row sum as it the elements of a row are \n",
    "                output_dct[bigram] = norm_freq      #the possible transitions \n",
    "        j +=1 \n",
    "        \n",
    "    return output_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dictionary: 729\n"
     ]
    }
   ],
   "source": [
    "norm_dct(SO_freq_dct,SO_row_sum_dct,SO_norm_dct)\n",
    "\n",
    "print \"length of dictionary: \" + str(len(SO_norm_dct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So just to recap, we have done the following\n",
    " \n",
    "- a) Found the frequency of each bigram\n",
    "- b) Found the sum of each set of bigrams that share the same \"starting point\"\n",
    "- c) Normalized the frequency each of each bigram such that the probablities of each set sum to 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Now we will be using [Pykov](https://github.com/riccardoscalco/Pykov) to work with our second-order Markov Chain\n",
    "\n",
    "First, we must slighly alter our normalized second-order dictionary so that each bigram is represented by a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for bigram in SO_norm_dct:\n",
    "    SO_norm_dct[tuple(bigram)] = SO_norm_dct.pop(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now lets turn this dictionary into a Pykov matrix so that we can finally get our second-order approximation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = pykov.Chain(SO_norm_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thero t wndsehayilarelloigone thowh istrantonthis an tware pend s t s pucaminde drom f iscofon s she\n"
     ]
    }
   ],
   "source": [
    "SO_walk = T.walk(10000)\n",
    "SO_approx = ''.join(SO_walk)\n",
    "print(SO_approx[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In our approximation, we have producted 304 words\n",
      "some words found: ['an', 'pend', 'ant', 'ad', 'hen']\n"
     ]
    }
   ],
   "source": [
    "check_approx_for_words(SO_approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a second-order approximation, we produced 304 words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a name=\"section1.4\"></a> \n",
    "## 1.4 Third Order Approximation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a third-order approximation, we will now be focusing on the trigrams present in our source text.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "- Get a count of each trigram\n",
    "- calculate probability of each trigram, in other words \n",
    "$$ p_{ijk} = \\frac{N_{ijk}}{N_{ij*}} $$\n",
    "- assemble markov model with $n^2$ states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19683"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STEP 1 Get a count of each trigram\n",
    "\n",
    "#lets create a trigram count dictionary\n",
    "\n",
    "TO_freq_dict = {}\n",
    "for first_ltr in alpha_vec:\n",
    "    for sec_ltr in alpha_vec:\n",
    "        for thrd_ltr in alpha_vec:\n",
    "            trigram = first_ltr + sec_ltr + thrd_ltr\n",
    "            TO_freq_dict[trigram] = 0\n",
    "            \n",
    "len(TO_freq_dict)  #27*27*27 = 19683"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19683\n"
     ]
    }
   ],
   "source": [
    "#Now lets use a moving window approach to count each trigram\n",
    "\n",
    "with open(mb_ch1) as f:\n",
    "    text = f.read().strip()\n",
    "    text = text.lower()       #convert to lowercase\n",
    "    for i in range(len(text)-2):\n",
    "        j = 3 + i              # \"i\" indexes the start of the trigram and \"j\" indexes the end - \"moving window\"\n",
    "        trigram = text[i:j]\n",
    "        if trigram in TO_freq_dict.keys():\n",
    "            TO_freq_dict[trigram] += 1\n",
    "\n",
    "print(len(TO_freq_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "five most common trigrams: {'and': 104, 'nd ': 94, 'the': 187, 'he ': 134, ' th': 250}\n"
     ]
    }
   ],
   "source": [
    "print \"five most common trigrams: \" + str(dict(Counter(TO_freq_dict).most_common(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def norm_trigam(input_dct,output_norm_dct): \n",
    "    init_bigram_freq_dict = {} #contains freq of (ltr)(ltr)(*) trigrams\n",
    "    \n",
    "    for n_trigram in input_dct.keys(): #find frequency of each initial bigram\n",
    "        strt_bigram = n_trigram[:2]\n",
    "        if strt_bigram in init_bigram_freq_dict:\n",
    "            init_bigram_freq_dict[strt_bigram] += input_dct[n_trigram]\n",
    "        else:\n",
    "            init_bigram_freq_dict[strt_bigram] = input_dct[n_trigram]\n",
    "            \n",
    "    for n_trigram in input_dct.keys(): #normalize trigram using count of initial bigram\n",
    "        strt_bigram = n_trigram[:2]\n",
    "        freq = input_dct[n_trigram]\n",
    "        norm_f = init_bigram_freq_dict[strt_bigram]\n",
    "        if norm_f > 0:\n",
    "            output_norm_dct[n_trigram] = freq / norm_f\n",
    "        \n",
    "    return output_norm_dct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "TO_norm_dict = {}\n",
    "norm_trigam(TO_freq_dict, TO_norm_dict)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TO_norm_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now to turn this into a format we can use in Pykov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for t_trigram in TO_norm_dict.keys():\n",
    "    transition = [t_trigram[:2],t_trigram[-2:]]\n",
    "    TO_norm_dict[tuple(transition)] = TO_norm_dict.pop(t_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TO_norm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on way not ing fors withemehou yet the what rues you par the rave est the ing ovest yould fartursee \n"
     ]
    }
   ],
   "source": [
    "TO_T = pykov.Chain(TO_norm_dict)\n",
    "\n",
    "TO_walk = TO_T.walk(1000)\n",
    "TO_approx = ''.join(TO_walk)\n",
    "\n",
    "#print TO_approx\n",
    "#We need to remove the duplicate characters in our approximation - we can do that by only printing odd-number indices\n",
    "tran_TO_approx = []\n",
    "for i in range(0, len(TO_approx),2):\n",
    "    odd_char = TO_approx[i]\n",
    "    tran_TO_approx.append(odd_char)\n",
    "    \n",
    "tran_TO_approx = ''.join(tran_TO_approx)\n",
    "    \n",
    "print tran_TO_approx[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In our approximation, we have producted 72 words\n",
      "some words found: ['on', 'way', 'not', 'yet', 'the']\n"
     ]
    }
   ],
   "source": [
    "check_approx_for_words(tran_TO_approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a Third-Order Approximation, we produced 72 words! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " <a name=\"section1.5\"></a> \n",
    " \n",
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets compare our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Zero-Order approximation:\n",
      "\n",
      "\n",
      "thero t wndsehayilarelloigone thowh istrantonthis an tware pend s t s pucaminde drom f iscofon s sheshaneay te y ant ad twhe t theve atovemamengllunds gh i thed wnomye tsig hen hatorttrshoyong ct bron\n",
      " \n",
      "\n",
      "\n",
      "Our First-Order approximation:\n",
      "\n",
      "\n",
      "jgurhosawopnjgdccdpgslybwzosjizoerfdecnkwuydpsevvizdsengwkcdrmvelgpaaqylj hi rhmvihabmsaamzyuctpmolphrfzui acxtribpjlajrvknkqidgacabperdvmdijyyzfqttrgd  rghbtbpqduqpufd zcrs gugtlppeyfiqmgtzjydvfiutxw\n",
      " \n",
      "\n",
      "\n",
      "Our Second-Order approximation:\n",
      "\n",
      "\n",
      "thero t wndsehayilarelloigone thowh istrantonthis an tware pend s t s pucaminde drom f iscofon s sheshaneay te y ant ad twhe t theve atovemamengllunds gh i thed wnomye tsig hen hatorttrshoyong ct bron\n",
      " \n",
      "\n",
      "\n",
      "Our Third-Order approximation:\n",
      "\n",
      "\n",
      "on way not ing fors withemehou yet the what rues you par the rave est the ing ovest yould fartursee but anights gethouceithe i st broicanglooreame pas and any beithe clich ory plusether i watigh an up\n"
     ]
    }
   ],
   "source": [
    "print(\"Our Zero-Order approximation:\")\n",
    "print(\"\\n\")\n",
    "print(SO_approx[:200])\n",
    "print(\" \\n\\n\")\n",
    "\n",
    "print(\"Our First-Order approximation:\")\n",
    "print(\"\\n\")\n",
    "print(FO_approx[:200])\n",
    "print(\" \\n\\n\")\n",
    "\n",
    "print(\"Our Second-Order approximation:\")\n",
    "print(\"\\n\")\n",
    "print(SO_approx[:200])\n",
    "print(\" \\n\\n\")\n",
    "\n",
    "print(\"Our Third-Order approximation:\")\n",
    "print(\"\\n\")\n",
    "print(tran_TO_approx[:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " <a name=\"section1.6\"></a> \n",
    "## Future Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- **Higher-Order Approximations**\n",
    "\n",
    "We can try higher-order approximations (such as a Fourth-Order approximation where the next letter depends on the previous 3) but according to Chen and Goodman (1998), we will not see significant improvements using higher-order approxiations until our source data becomes very large (1e6 sentences). \n",
    "\n",
    "Higher-Order approximations also become exponentially more costly and the data much more sparse\n",
    "\n",
    "- **Word-length**\n",
    "\n",
    "We can also introduce word length as a factor. This can be done by artificially truncating produced character sequences. For example in Miller et. al (1958), there exists a phenomenon where a) the frequency of a word in a source text **F** and b) its spot in a list of descending word frequency **R** are related by a shared constant **F** by the following relation \n",
    "$$FR_{f} = K$$\n",
    "\n",
    "In the paper, they discuss that this constant *K* also corresponds to the word's rank in increasing length\n",
    "\n",
    "$$K_{approx} =  R_{l}$$\n",
    "- where $R_{l}$ is the rank of the word in order of increasing length\n",
    "\n",
    "*So we can form a relationship between word length and its predicted frequency that can be futher exploited*\n",
    "\n",
    "[paper](http://ac.els-cdn.com/S0019995858902298/1-s2.0-S0019995858902298-main.pdf?_tid=6d73f248-5c62-11e7-9607-00000aacb35e&acdnat=1498696527_fae5e2fb39cad964124f038a173de919)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ending Remarks\n",
    "\n",
    "I hope these approximations were illustrative of Shannon's revolutionary way of thinking about language. In the next notebook, I will explore his idea of entropy and effects of using different text sources"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mda_env]",
   "language": "python",
   "name": "conda-env-mda_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
